{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce7cdaf9",
   "metadata": {},
   "source": [
    "# Turn datasets into eval-ready format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b282b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ───────▸ EDIT THESE ──────────────────────────────────────────────────────────\n",
    "parquet_files = [...] # List of Parquet files to combine\n",
    "output_csv = \"\" # Path to output CSV file\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Read each Parquet into a DataFrame\n",
    "dfs = []\n",
    "for p in parquet_files:\n",
    "    print(f\"Loading {p} …\")\n",
    "    dfs.append(pd.read_parquet(p))\n",
    "\n",
    "# Concatenate all together\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"Combined DataFrame has {combined_df.shape[0]} rows and {combined_df.shape[1]} columns.\")\n",
    "\n",
    "# Write out to CSV\n",
    "combined_df.to_csv(output_csv, index=False)\n",
    "print(f\"Wrote CSV to: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07947b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ───────▸ EDIT THESE ──────────────────────────────────────────────────────────\n",
    "input_csv   = \"\"      # your input file\n",
    "output_csv  = \"\"    # where to save the reformatted CSV\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# 1. Load your existing data\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# 2. Build the prompt and rename columns\n",
    "df['prompt']     = df['label'].apply(lambda lbl: f\"How many {lbl} are in this picture?\")\n",
    "df['num_points'] = df['count']\n",
    "\n",
    "# 3. Select & reorder into the “old” format + sha\n",
    "out_df = df[['image_url', 'prompt', 'num_points', 'image_sha256']]\n",
    "\n",
    "# 4. Save and peek\n",
    "out_df.to_csv(output_csv, index=False)\n",
    "print(f\"Saved {len(out_df)} rows to {output_csv}\")\n",
    "out_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d67245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ─────── EDIT THESE ────────────────────────────────────────────────────────────\n",
    "input_files = [] # List of CSV files to combine\n",
    "output_csv = \"dataset.csv\"\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "all_dfs = []\n",
    "for fn in input_files:\n",
    "    df = pd.read_csv(fn)\n",
    "\n",
    "    # (1) only integer answers\n",
    "    ans_num = pd.to_numeric(df['answer'], errors='coerce')\n",
    "    mask = ans_num.notnull() & (ans_num % 1 == 0)\n",
    "    df = df.loc[mask].copy()\n",
    "    df['answer'] = ans_num[mask].astype(int)\n",
    "\n",
    "    # (2) build URL\n",
    "    df['image_url'] = (\n",
    "        \"https://storage.googleapis.com/geckonum_t2i_benchmark/\"\n",
    "        + df['model'] + \"/\" + df['image_id'] + \".png\"\n",
    "    )\n",
    "\n",
    "    # (3) prompt = question_id\n",
    "    df['prompt'] = df['question']\n",
    "\n",
    "    # (4) select columns\n",
    "    all_dfs.append(df[['image_url', 'prompt', 'answer']])\n",
    "\n",
    "# concatenate\n",
    "combined = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# (5) collapse duplicates, keep first annotator’s answer\n",
    "unique = combined.drop_duplicates(subset=['image_url','prompt'], keep='first')\n",
    "\n",
    "# (6) write out\n",
    "unique.to_csv(output_csv, index=False)\n",
    "print(f\"Wrote {len(unique)} unique rows to {output_csv}\")\n",
    "\n",
    "# show a sample\n",
    "unique.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac094a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ───────▸ EDIT THESE ──────────────────────────────────────────────────────────\n",
    "csv_path    = Path(\"/cluster/project/sachan/pmlr/grounding-vlms/eval/datasets/TallyQA/original_dataset.csv\")       # CSV with column of relative paths\n",
    "col_name    = \"file_name\"                     # that column (e.g. 'val2014/XXX.jpg')\n",
    "images_dir  = Path(\"/cluster/project/sachan/pmlr/grounding-vlms/eval/datasets/TallyQA/images\")   # contains subfolders like 'val2014/'\n",
    "exts        = {\".jpg\", \".png\"}                # restrict to these extensions or None\n",
    "dry_run     = False                         # True: print only; False: actually delete\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Load allowed relative‑paths\n",
    "df = pd.read_csv(csv_path)\n",
    "if col_name not in df.columns:\n",
    "    raise ValueError(f\"Column {col_name!r} not found in {csv_path}\")\n",
    "allowed = set(df[col_name].astype(str))\n",
    "\n",
    "# Walk recursively\n",
    "for img_path in images_dir.rglob(\"*\"):\n",
    "    if not img_path.is_file():\n",
    "        continue\n",
    "    if exts and img_path.suffix.lower() not in exts:\n",
    "        continue\n",
    "\n",
    "    # Compute path *relative* to the root, with forward‑slashes\n",
    "    rel = img_path.relative_to(images_dir).as_posix()\n",
    "\n",
    "    # If that rel‑path isn't in CSV, delete (or dry‑run)\n",
    "    if rel not in allowed:\n",
    "        if dry_run:\n",
    "            print(f\"[DRY RUN] would delete: {rel}\")\n",
    "        else:\n",
    "            print(f\"Deleting: {rel}\")\n",
    "            img_path.unlink()\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9706333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load your CSV\n",
    "df = pd.read_csv('/Users/timeilers/Documents/ETH/Material/AICP/Results/data/FSC-147_dataset.csv')  # ← change to your CSV filename\n",
    "\n",
    "# 2. Extract the phrase between \"How many\" and \"are in this picture\"\n",
    "df['subject'] = df['prompt'].str.extract(r'How many\\s+(.+?)\\s+are in this picture')\n",
    "\n",
    "# 3. (Optional) Save to a new CSV\n",
    "df.to_csv('/Users/timeilers/Documents/ETH/Material/AICP/Results/data/new_FSC-147_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bc4790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# load spaCy’s English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "df = pd.read_csv('/Users/timeilers/Documents/ETH/Material/AICP/Results/data/TallyQA_dataset.csv')\n",
    "\n",
    "def extract_head(text):\n",
    "    doc = nlp(text)\n",
    "    # grab the first noun chunk whose root is a NOUN or PROPN\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if chunk.root.pos_ in (\"NOUN\", \"PROPN\"):\n",
    "            return chunk.root.text\n",
    "    return None\n",
    "\n",
    "df[\"label\"] = df[\"prompt\"].apply(extract_head)\n",
    "\n",
    "df.to_csv('/Users/timeilers/Documents/ETH/Material/AICP/Results/data/new_TallyQA_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40405aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling for TallyQA\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/cluster/project/sachan/pmlr/grounding-vlms/eval/datasets/TallyQA/original_dataset.csv')\n",
    "\n",
    "df['prefix'] = df['file_name'].str.split('/', n=1).str[0]\n",
    "df['strata'] = df['prefix'] + '___' + df['truth'].astype(str) + '___' + df['label']\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
    "\n",
    "train_idx, sample_idx = next(sss.split(df, df['strata']))\n",
    "\n",
    "sample_df = df.iloc[sample_idx].drop(columns=['strata', 'prefix']).reset_index(drop=True)\n",
    "sample_df.to_csv('/cluster/project/sachan/pmlr/grounding-vlms/eval/datasets/TallyQA/dataset_sampled.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5826fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling for GeckoNum\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/cluster/project/sachan/pmlr/grounding-vlms/eval/datasets/GeckoNum/original_dataset.csv')\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "train_idx, sample_idx = next(sss.split(df, df['truth']))\n",
    "\n",
    "sample_df = df.iloc[sample_idx].reset_index(drop=True)\n",
    "sample_df.to_csv('/cluster/project/sachan/pmlr/grounding-vlms/eval/datasets/GeckoNum/dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7c58187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted rows for idx 15024:\n",
      "         idx  result  raw_result\n",
      "15021  15024       0           0\n",
      "Deleted rows for idx 8284:\n",
      "       idx  result  raw_result\n",
      "8281  8284       0           0\n",
      "Deleted rows for idx 2404:\n",
      "       idx  result  raw_result\n",
      "2401  2404       0           0\n",
      "Deleted rows for idx 10456:\n",
      "         idx  result  raw_result\n",
      "10453  10456       0           0\n",
      "Deleted rows for idx 2594:\n",
      "       idx  result  raw_result\n",
      "2591  2594       0           0\n",
      "Deleted rows for idx 5762:\n",
      "       idx  result  raw_result\n",
      "5759  5762       0           0\n",
      "Deleted rows for idx 11164:\n",
      "         idx  result  raw_result\n",
      "11161  11164       0           0\n",
      "Deleted rows for idx 9789:\n",
      "       idx  result  raw_result\n",
      "9786  9789       0           0\n",
      "Deleted rows for idx 14301:\n",
      "         idx  result  raw_result\n",
      "14298  14301       0           0\n",
      "Deleted rows for idx 21029:\n",
      "         idx  result  raw_result\n",
      "21026  21029       0           0\n",
      "Deleted rows for idx 8038:\n",
      "       idx  result  raw_result\n",
      "8035  8038       0           0\n",
      "Deleted rows for idx 17604:\n",
      "         idx  result  raw_result\n",
      "17601  17604       0           0\n",
      "Deleted rows for idx 18559:\n",
      "         idx  result  raw_result\n",
      "18556  18559       0           0\n",
      "Deleted rows for idx 11346:\n",
      "         idx  result  raw_result\n",
      "11343  11346       0           0\n",
      "Deleted rows for idx 21198:\n",
      "         idx  result  raw_result\n",
      "21195  21198       0           0\n",
      "Deleted rows for idx 9543:\n",
      "       idx  result  raw_result\n",
      "9540  9543       0           0\n",
      "Deleted rows for idx 13414:\n",
      "         idx  result  raw_result\n",
      "13411  13414       0           0\n",
      "Deleted rows for idx 9971:\n",
      "       idx  result  raw_result\n",
      "9968  9971       0           0\n",
      "Deleted rows for idx 417:\n",
      "     idx  result  raw_result\n",
      "414  417       0           0\n",
      "Deleted rows for idx 14349:\n",
      "         idx  result  raw_result\n",
      "14346  14349       0           0\n",
      "Deleted rows for idx 14351:\n",
      "         idx  result  raw_result\n",
      "14348  14351       0           0\n",
      "Deleted rows for idx 4473:\n",
      "       idx  result  raw_result\n",
      "4470  4473       0           0\n",
      "Deleted rows for idx 7743:\n",
      "       idx  result  raw_result\n",
      "7740  7743       0           0\n",
      "Deleted rows for idx 5005:\n",
      "       idx  result  raw_result\n",
      "5002  5005       0           0\n",
      "Deleted rows for idx 14809:\n",
      "         idx  result  raw_result\n",
      "14806  14809       0           0\n",
      "Deleted rows for idx 10133:\n",
      "         idx  result  raw_result\n",
      "10130  10133       0           0\n",
      "Deleted rows for idx 3389:\n",
      "       idx  result  raw_result\n",
      "3386  3389       0           0\n",
      "Deleted rows for idx 19537:\n",
      "         idx  result  raw_result\n",
      "19534  19537       0           0\n",
      "Deleted rows for idx 6554:\n",
      "       idx  result  raw_result\n",
      "6551  6554       0           0\n",
      "Deleted rows for idx 18291:\n",
      "         idx  result  raw_result\n",
      "18288  18291       0           0\n",
      "Deleted rows for idx 12381:\n",
      "         idx  result  raw_result\n",
      "12378  12381       0           0\n",
      "Deleted rows for idx 14642:\n",
      "         idx  result  raw_result\n",
      "14639  14642       0           0\n",
      "Deleted rows for idx 8395:\n",
      "       idx  result  raw_result\n",
      "8392  8395       0           0\n",
      "Deleted rows for idx 1193:\n",
      "       idx  result  raw_result\n",
      "1190  1193       0           0\n",
      "Deleted rows for idx 18656:\n",
      "         idx  result  raw_result\n",
      "18653  18656       0           0\n",
      "Deleted rows for idx 12318:\n",
      "         idx  result  raw_result\n",
      "12315  12318       0           0\n",
      "Deleted rows for idx 3781:\n",
      "       idx  result  raw_result\n",
      "3778  3781       0           0\n",
      "Deleted rows for idx 1632:\n",
      "       idx  result  raw_result\n",
      "1629  1632       0           0\n",
      "Deleted rows for idx 3921:\n",
      "       idx  result  raw_result\n",
      "3918  3921       0           0\n",
      "Deleted rows for idx 20865:\n",
      "         idx  result  raw_result\n",
      "20862  20865       0           0\n",
      "Deleted rows for idx 14615:\n",
      "         idx  result  raw_result\n",
      "14612  14615       0           0\n",
      "Deleted rows for idx 20857:\n",
      "         idx  result  raw_result\n",
      "20854  20857       0           0\n",
      "Deleted rows for idx 5114:\n",
      "       idx  result  raw_result\n",
      "5111  5114       0           0\n",
      "Deleted rows for idx 17843:\n",
      "         idx  result  raw_result\n",
      "17840  17843       0           0\n",
      "Deleted rows for idx 19618:\n",
      "         idx  result  raw_result\n",
      "19615  19618       0           0\n",
      "Deleted rows for idx 19955:\n",
      "         idx  result  raw_result\n",
      "19952  19955       0           0\n",
      "Deleted rows for idx 14251:\n",
      "         idx  result  raw_result\n",
      "14248  14251       0           0\n",
      "Deleted rows for idx 14254:\n",
      "         idx  result  raw_result\n",
      "14251  14254       0           0\n",
      "Deleted rows for idx 5093:\n",
      "       idx  result  raw_result\n",
      "5090  5093       0           0\n",
      "Deleted rows for idx 11008:\n",
      "         idx  result  raw_result\n",
      "11005  11008       0           0\n",
      "Deleted rows for idx 15759:\n",
      "         idx  result  raw_result\n",
      "15756  15759       0           0\n",
      "Deleted rows for idx 2320:\n",
      "       idx  result  raw_result\n",
      "2317  2320       0           0\n",
      "Deleted rows for idx 15758:\n",
      "         idx  result  raw_result\n",
      "15755  15758       0           0\n",
      "Deleted rows for idx 17111:\n",
      "         idx  result  raw_result\n",
      "17108  17111       0           0\n",
      "Deleted rows for idx 12559:\n",
      "         idx  result  raw_result\n",
      "12556  12559       0           0\n",
      "Deleted rows for idx 11705:\n",
      "         idx  result  raw_result\n",
      "11702  11705       0           0\n",
      "Deleted rows for idx 15767:\n",
      "         idx  result  raw_result\n",
      "15764  15767       0           0\n",
      "Deleted rows for idx 3630:\n",
      "       idx  result  raw_result\n",
      "3627  3630       0           0\n",
      "Deleted rows for idx 7718:\n",
      "       idx  result  raw_result\n",
      "7715  7718       0           0\n",
      "Deleted rows for idx 4985:\n",
      "       idx  result  raw_result\n",
      "4982  4985       0           0\n",
      "Deleted rows for idx 4917:\n",
      "       idx  result  raw_result\n",
      "4914  4917       0           0\n",
      "Deleted rows for idx 3577:\n",
      "       idx  result  raw_result\n",
      "3574  3577       0           0\n",
      "Deleted rows for idx 1267:\n",
      "       idx  result  raw_result\n",
      "1264  1267       0           0\n",
      "Deleted rows for idx 17090:\n",
      "         idx  result  raw_result\n",
      "17087  17090       0           0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/cluster/project/sachan/pmlr/grounding-vlms/eval/valid_results/o4-mini/TallyQA_results.csv')\n",
    "\n",
    "# Series of counts per idx\n",
    "counts = df['idx'].value_counts()\n",
    "\n",
    "# index values with count > 1\n",
    "dup_values = counts[counts > 1].index.tolist()\n",
    "\n",
    "# Of the duplicate rows, keep the one that appears last in the CSV and print all deleted rows\n",
    "for idx in dup_values:\n",
    "    # Get all rows with the same index value\n",
    "    duplicate_rows = df[df['idx'] == idx]\n",
    "    \n",
    "    # Keep the last row and drop the rest\n",
    "    df = df.drop(duplicate_rows.index[:-1])\n",
    "    \n",
    "    # Print the deleted rows\n",
    "    print(f\"Deleted rows for idx {idx}:\")\n",
    "    print(duplicate_rows.iloc[:-1])\n",
    "\n",
    "df.to_csv('/cluster/project/sachan/pmlr/grounding-vlms/eval/valid_results/o4-mini/TallyQA_results_new.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
