{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce7cdaf9",
   "metadata": {},
   "source": [
    "# Turn datasets into eval-ready format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b282b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ───────▸ EDIT THESE ──────────────────────────────────────────────────────────\n",
    "parquet_files = [...] # List of Parquet files to combine\n",
    "output_csv = \"\" # Path to output CSV file\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Read each Parquet into a DataFrame\n",
    "dfs = []\n",
    "for p in parquet_files:\n",
    "    print(f\"Loading {p} …\")\n",
    "    dfs.append(pd.read_parquet(p))\n",
    "\n",
    "# Concatenate all together\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"Combined DataFrame has {combined_df.shape[0]} rows and {combined_df.shape[1]} columns.\")\n",
    "\n",
    "# Write out to CSV\n",
    "combined_df.to_csv(output_csv, index=False)\n",
    "print(f\"Wrote CSV to: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07947b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ───────▸ EDIT THESE ──────────────────────────────────────────────────────────\n",
    "input_csv   = \"\"      # your input file\n",
    "output_csv  = \"\"    # where to save the reformatted CSV\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# 1. Load your existing data\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# 2. Build the prompt and rename columns\n",
    "df['prompt']     = df['label'].apply(lambda lbl: f\"How many {lbl} are in this picture?\")\n",
    "df['num_points'] = df['count']\n",
    "\n",
    "# 3. Select & reorder into the “old” format + sha\n",
    "out_df = df[['image_url', 'prompt', 'num_points', 'image_sha256']]\n",
    "\n",
    "# 4. Save and peek\n",
    "out_df.to_csv(output_csv, index=False)\n",
    "print(f\"Saved {len(out_df)} rows to {output_csv}\")\n",
    "out_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d67245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ─────── EDIT THESE ────────────────────────────────────────────────────────────\n",
    "input_files = [] # List of CSV files to combine\n",
    "output_csv = \"dataset.csv\"\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "all_dfs = []\n",
    "for fn in input_files:\n",
    "    df = pd.read_csv(fn)\n",
    "\n",
    "    # (1) only integer answers\n",
    "    ans_num = pd.to_numeric(df['answer'], errors='coerce')\n",
    "    mask = ans_num.notnull() & (ans_num % 1 == 0)\n",
    "    df = df.loc[mask].copy()\n",
    "    df['answer'] = ans_num[mask].astype(int)\n",
    "\n",
    "    # (2) build URL\n",
    "    df['image_url'] = (\n",
    "        \"https://storage.googleapis.com/geckonum_t2i_benchmark/\"\n",
    "        + df['model'] + \"/\" + df['image_id'] + \".png\"\n",
    "    )\n",
    "\n",
    "    # (3) prompt = question_id\n",
    "    df['prompt'] = df['question']\n",
    "\n",
    "    # (4) select columns\n",
    "    all_dfs.append(df[['image_url', 'prompt', 'answer']])\n",
    "\n",
    "# concatenate\n",
    "combined = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# (5) collapse duplicates, keep first annotator’s answer\n",
    "unique = combined.drop_duplicates(subset=['image_url','prompt'], keep='first')\n",
    "\n",
    "# (6) write out\n",
    "unique.to_csv(output_csv, index=False)\n",
    "print(f\"Wrote {len(unique)} unique rows to {output_csv}\")\n",
    "\n",
    "# show a sample\n",
    "unique.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac094a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ───────▸ EDIT THESE ──────────────────────────────────────────────────────────\n",
    "csv_path    = Path(\"/cluster/project/sachan/pmlr/grounding-vlms/eval/datasets/TallyQA/original_dataset.csv\")       # CSV with column of relative paths\n",
    "col_name    = \"file_name\"                     # that column (e.g. 'val2014/XXX.jpg')\n",
    "images_dir  = Path(\"/cluster/project/sachan/pmlr/grounding-vlms/eval/datasets/TallyQA/images\")   # contains subfolders like 'val2014/'\n",
    "exts        = {\".jpg\", \".png\"}                # restrict to these extensions or None\n",
    "dry_run     = False                         # True: print only; False: actually delete\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Load allowed relative‑paths\n",
    "df = pd.read_csv(csv_path)\n",
    "if col_name not in df.columns:\n",
    "    raise ValueError(f\"Column {col_name!r} not found in {csv_path}\")\n",
    "allowed = set(df[col_name].astype(str))\n",
    "\n",
    "# Walk recursively\n",
    "for img_path in images_dir.rglob(\"*\"):\n",
    "    if not img_path.is_file():\n",
    "        continue\n",
    "    if exts and img_path.suffix.lower() not in exts:\n",
    "        continue\n",
    "\n",
    "    # Compute path *relative* to the root, with forward‑slashes\n",
    "    rel = img_path.relative_to(images_dir).as_posix()\n",
    "\n",
    "    # If that rel‑path isn't in CSV, delete (or dry‑run)\n",
    "    if rel not in allowed:\n",
    "        if dry_run:\n",
    "            print(f\"[DRY RUN] would delete: {rel}\")\n",
    "        else:\n",
    "            print(f\"Deleting: {rel}\")\n",
    "            img_path.unlink()\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9706333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load your CSV\n",
    "df = pd.read_csv('/Users/timeilers/Documents/ETH/Material/AICP/Results/data/FSC-147_dataset.csv')  # ← change to your CSV filename\n",
    "\n",
    "# 2. Extract the phrase between \"How many\" and \"are in this picture\"\n",
    "df['subject'] = df['prompt'].str.extract(r'How many\\s+(.+?)\\s+are in this picture')\n",
    "\n",
    "# 3. (Optional) Save to a new CSV\n",
    "df.to_csv('/Users/timeilers/Documents/ETH/Material/AICP/Results/data/new_FSC-147_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bc4790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# load spaCy’s English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "df = pd.read_csv('/Users/timeilers/Documents/ETH/Material/AICP/Results/data/TallyQA_dataset.csv')\n",
    "\n",
    "def extract_head(text):\n",
    "    doc = nlp(text)\n",
    "    # grab the first noun chunk whose root is a NOUN or PROPN\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if chunk.root.pos_ in (\"NOUN\", \"PROPN\"):\n",
    "            return chunk.root.text\n",
    "    return None\n",
    "\n",
    "df[\"label\"] = df[\"prompt\"].apply(extract_head)\n",
    "\n",
    "df.to_csv('/Users/timeilers/Documents/ETH/Material/AICP/Results/data/new_TallyQA_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40405aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling for TallyQA\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/cluster/project/sachan/pmlr/grounding-vlms/eval/datasets/TallyQA/original_dataset.csv')\n",
    "\n",
    "df['prefix'] = df['file_name'].str.split('/', n=1).str[0]\n",
    "df['strata'] = df['prefix'] + '___' + df['truth'].astype(str) + '___' + df['label']\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
    "\n",
    "train_idx, sample_idx = next(sss.split(df, df['strata']))\n",
    "\n",
    "sample_df = df.iloc[sample_idx].drop(columns=['strata', 'prefix']).reset_index(drop=True)\n",
    "sample_df.to_csv('/cluster/project/sachan/pmlr/grounding-vlms/eval/datasets/TallyQA/dataset_sampled.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5826fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling for GeckoNum\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/cluster/project/sachan/pmlr/grounding-vlms/eval/datasets/GeckoNum/original_dataset.csv')\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "train_idx, sample_idx = next(sss.split(df, df['truth']))\n",
    "\n",
    "sample_df = df.iloc[sample_idx].reset_index(drop=True)\n",
    "sample_df.to_csv('/cluster/project/sachan/pmlr/grounding-vlms/eval/datasets/GeckoNum/dataset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
