engine: huggingface
model: google/gemma-3-4b-it
processor: google/gemma-3-4b-it
#device_map: auto
#torch_dtype: bfloat16 in the docs it said bfloat16 but lets see if it still works(hf_model hardcoder it to float16)
system_prompt: You are a helpful assistant.
max_new_tokens: 256
do_sample: false
